{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-programming Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Hadamard matrix product?\n",
    "\n",
    "Answer:\n",
    "\n",
    "The Hadamard matrix product, also known as the Hadamard product or element-wise product, is a mathematical operation performed on two matrices or arrays of the same shape. In this operation, each element of one matrix is multiplied by the corresponding element of the other matrix. The result is a new matrix or array of the same shape, where each element at position (i, j) is the product of the elements at the same position in the original matrices.\n",
    "\n",
    "Mathematically, if you have two matrices A and B of the same shape, the Hadamard product (denoted by ⊙) is calculated as follows:\n",
    "\n",
    "![image](1.png)\n",
    "\n",
    "The Hadamard product is different from matrix multiplication (dot product), where the elements of the resulting matrix are computed based on a combination of elements from both input matrices using summation. In contrast, the Hadamard product operates element-wise and is used in various mathematical and computational applications, including image processing, element-wise operations in neural networks, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Describe matrix multiplication? \n",
    "\n",
    "Answer:\n",
    "\n",
    "Matrix multiplication, also known as matrix product, is a fundamental operation in linear algebra used to combine two matrices to produce a third matrix. Matrix multiplication is defined for matrices of compatible dimensions, where the number of columns in the first matrix must be equal to the number of rows in the second matrix. If matrices \n",
    "A and B are being multiplied to produce matrix C, the dimensions must satisfy the condition: \n",
    "A is of shape m×n, and B is of shape n×p, where m, n, and p are positive integers.\n",
    "The resulting matrix C will have dimensions m×p. Each element C[i][j] of the resulting matrix is calculated as the sum of the products of elements from the corresponding rows of A and columns of B:\n",
    "\n",
    "![image](2.png)\n",
    "\n",
    "Here's a step-by-step description of how matrix multiplication is performed:\n",
    "\n",
    "1. Take the element in the ith row and jth column of matrix C.\n",
    "2. Multiply each element in the ith row of matrix A by the corresponding element in the jth column of matrix B.\n",
    "3. Sum all the products obtained in step 2 to calculate the value of C[i][j].\n",
    "\n",
    "Matrix multiplication is not commutative, meaning that the order in which matrices are multiplied matters. In other words, A⋅B is not necessarily equal to B⋅A unless special conditions are met. It is an essential operation in various mathematical and scientific applications, including solving systems of linear equations, transformations, computer graphics, and machine learning, particularly in neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is transpose matrix and vector?\n",
    "\n",
    "Answer:\n",
    "\n",
    "A transpose matrix, also known as the transpose of a matrix, is obtained by interchanging the rows and columns of the original matrix. If you have a matrix A of dimensions m×n, the transpose of A, also denoted as A^T or A', will have dimensions n×m, and its elements will be arranged such that the rows of the original matrix become columns in the transposed matrix, and vice versa.\n",
    "\n",
    "For each element A[i][j] in the original matrix A, the corresponding element in the transposed matrix A' is [j][i]. In other words, the element at row i and column j in A becomes the element at row j and column i in A'.\n",
    "\n",
    "Here's a simple example to illustrate matrix transposition:\n",
    "\n",
    "![image](3.png)\n",
    "\n",
    "Matrix transposition is a fundamental operation in linear algebra with various applications in mathematics, physics, engineering, and computer science.\n",
    "\n",
    "A transposed vector is a vector that has been transposed into a matrix format. In the context of vectors, a vector is typically represented as a column vector, meaning it is a single column of numbers. Transposing a vector means converting it into a row vector, which is a single row of numbers. This transformation is done by changing the orientation of the vector without altering its elements.\n",
    "\n",
    "For example, consider the following column vector v:\n",
    "\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\\]\n",
    "The transposed vector \\(v^T\\) is a row vector:\n",
    "\\[v^T = \\begin{bmatrix}\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix}\\]\n",
    "The notation \\(v^T\\) indicates that the vector \\(v\\) has been transposed into a row vector.\n",
    "Transpose operations are commonly used in linear algebra, matrix calculations, and various mathematical and computational applications. They play a crucial role in transformations, solving systems of linear equations, and many other mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Describe loss (cost or error) function in neural network\n",
    "\n",
    "Answer:\n",
    "\n",
    "In a neural network, a loss function, also known as a cost function or error function, is a mathematical function that quantifies the difference between the predicted output (or activations) of the network and the actual target values for a given set of input data. The primary purpose of a loss function is to measure how well or poorly the neural network is performing in terms of its ability to make accurate predictions.\n",
    "\n",
    "The choice of the appropriate loss function depends on the type of task the neural network is designed for. Different tasks, such as classification, regression, or generative modeling, require different types of loss functions. Here are some common types of loss functions used in neural networks:\n",
    "\n",
    "1. Mean Squared Error (MSE) Loss:\n",
    "\n",
    "Used for regression tasks.\n",
    "Calculates the average of the squared differences between the predicted values and the actual target values.\n",
    "\n",
    "![image](4.png)\n",
    "\n",
    "where y of i is the actual target, ^y of i is the predicted value, and n is the number of samples.\n",
    "\n",
    "2. Cross-Entropy Loss (Log Loss):\n",
    "\n",
    "Used for classification tasks, especially in binary and multiclass classification.\n",
    "Measures the dissimilarity between the predicted class probabilities and the true class labels.\n",
    "It is often used with softmax activation in the output layer.\n",
    "\n",
    "![image](5.png)\n",
    "\n",
    "where y of i is the true class probability, ^y of i is the predicted class probability, and C is the number of classes.\n",
    "\n",
    "3. Hinge Loss (SVM Loss):\n",
    "\n",
    "Used in support vector machines (SVMs) and binary classification tasks.\n",
    "Encourages the correct class score to be higher than the sum of the incorrect class scores by a certain margin.\n",
    "\n",
    "![image](6.png)\n",
    "\n",
    "where y is the true class label (-1 or 1), and ^y is the predicted score.\n",
    "\n",
    "4. Kullback-Leibler (KL) Divergence Loss:\n",
    "\n",
    "Used in probabilistic models and variational autoencoders (VAEs).\n",
    "Measures the difference between two probability distributions, such as a learned distribution and a target distribution.\n",
    "\n",
    "![image](7.png)\n",
    "\n",
    "where p of i is the true distribution, and q of i is the predicted distribution.\n",
    "\n",
    "The goal during training is to minimize the value of the loss function. This optimization process, often referred to as backpropagation, involves adjusting the neural network's parameters (weights and biases) using gradient descent or other optimization algorithms. The choice of the loss function and optimization method depends on the specific problem being solved and the architecture of the neural network.\n",
    "\n",
    "In summary, a loss function quantifies the error or discrepancy between the neural network's predictions and the actual target values, guiding the training process to improve the model's accuracy and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Describe the foundations of neural network supervised training.\n",
    "\n",
    "Answer:\n",
    "\n",
    "The foundations of supervised training in neural networks form the basis for how these models learn from data to make predictions. Supervised training is a process where the neural network is provided with a labeled dataset, consisting of input data and corresponding target labels or values. The goal is for the network to learn a mapping from inputs to outputs that can generalize to unseen data. Here are the key components of supervised training in neural networks:\n",
    "\n",
    "Dataset Preparation:\n",
    "\n",
    "In supervised learning, you start with a labeled dataset that consists of input samples (features) and their corresponding target labels or values.\n",
    "The dataset is typically divided into two subsets: a training set used for training the model and a validation or test set used to evaluate the model's performance.\n",
    "Neural Network Architecture:\n",
    "\n",
    "You define the architecture of the neural network, which includes the number and structure of layers, the types of activation functions, and the number of neurons or units in each layer.\n",
    "The architecture depends on the specific problem you are trying to solve (e.g., classification, regression) and the complexity of the data.\n",
    "Initialization:\n",
    "\n",
    "Neural network parameters, including weights and biases, are initialized with small random values or specific techniques like Xavier/Glorot initialization.\n",
    "Proper initialization helps the network start learning effectively and avoids getting stuck in local minima.\n",
    "Forward Propagation:\n",
    "\n",
    "During training, input data is passed through the neural network in a forward direction.\n",
    "Each neuron computes a weighted sum of its inputs, applies an activation function, and passes the result to the next layer.\n",
    "The final layer's output represents the predicted values or class probabilities.\n",
    "Loss Function:\n",
    "\n",
    "A loss function (also known as a cost function or error function) quantifies the difference between the predicted outputs and the actual target labels.\n",
    "Common loss functions include mean squared error (MSE) for regression and cross-entropy loss for classification.\n",
    "Backpropagation:\n",
    "\n",
    "After forward propagation, the network calculates the loss.\n",
    "Backpropagation is the process of computing the gradients of the loss with respect to the model's parameters (weights and biases) using the chain rule.\n",
    "These gradients guide the optimization algorithm to update the parameters in a way that reduces the loss.\n",
    "Optimization Algorithm:\n",
    "\n",
    "An optimization algorithm (e.g., gradient descent, Adam, RMSprop) adjusts the model's parameters to minimize the loss.\n",
    "The learning rate determines the step size for parameter updates, and other hyperparameters may influence the optimization process.\n",
    "Training Iterations (Epochs):\n",
    "\n",
    "The training process consists of multiple iterations called epochs.\n",
    "During each epoch, the entire training dataset is processed by the network, and the parameters are updated.\n",
    "The number of epochs is a hyperparameter that affects how long the training process continues.\n",
    "Validation and Testing:\n",
    "\n",
    "Periodically, the model's performance is evaluated on a separate validation or test dataset that it has not seen during training.\n",
    "Metrics such as accuracy, mean squared error, or others are used to assess the model's generalization to new data.\n",
    "Early Stopping:\n",
    "\n",
    "To prevent overfitting, you can implement early stopping, where training is halted when the validation loss starts increasing.\n",
    "Model Deployment:\n",
    "\n",
    "Once the model achieves satisfactory performance on the validation set, it can be deployed to make predictions on new, unseen data.\n",
    "In summary, supervised training in neural networks involves setting up the network architecture, initializing parameters, propagating data forward through the network, computing a loss, backpropagating gradients, and iteratively optimizing the model. The process continues until the model converges to a solution or meets predefined stopping criteria. The key is to find a balance between model complexity and generalization to achieve the best performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Describe forward propagation and backpropagation.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Forward propagation and backpropagation are fundamental processes in training neural networks. They are part of the supervised learning framework and are used to update the model's parameters (weights and biases) based on the provided input data and target labels. Here's a detailed description of both forward propagation and backpropagation:\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "Input Data: Forward propagation begins with the input data. Each input sample is represented as a feature vector, and these vectors are fed into the neural network.\n",
    "\n",
    "Layer Computation: The input data is passed through the network layer by layer, from the input layer to the output layer.\n",
    "\n",
    "For each layer, the following steps are performed:\n",
    "Weighted Sum: The input data is linearly transformed by computing the weighted sum of the inputs. Each neuron in the layer has its set of weights, and this sum includes the weights and a bias term.\n",
    "Activation Function: The weighted sum is passed through an activation function, which introduces non-linearity into the network. Common activation functions include ReLU, sigmoid, and tanh.\n",
    "Output: The result of the activation function becomes the output of the current layer and serves as input to the next layer.\n",
    "Final Output: After propagating through all layers, the final output of the neural network is obtained. This output can represent predictions for tasks like classification or regression.\n",
    "\n",
    "Loss Calculation: A loss function (also known as a cost function) is used to compute the error between the predicted output and the actual target labels. The loss function quantifies how far off the predictions are from the truth.\n",
    "\n",
    "Backpropagation:\n",
    "\n",
    "Gradient Calculation: Backpropagation starts with the calculation of gradients of the loss with respect to the model's parameters (weights and biases). These gradients measure how much the loss would change if each parameter were adjusted slightly.\n",
    "\n",
    "Error Propagation: The gradients are propagated backward through the network, starting from the output layer and moving toward the input layer. This is done using the chain rule of calculus.\n",
    "\n",
    "For each layer, the following steps are performed:\n",
    "Gradient of Activation: The gradient of the activation function is computed for the layer's output. This measures how sensitive the output is to small changes.\n",
    "Error Propagation: The gradients from the previous layer and the gradient of activation are combined to calculate the gradients for the weights and biases in the current layer.\n",
    "Parameter Update: The model's parameters (weights and biases) are updated using the computed gradients. The learning rate determines the step size of these updates.\n",
    "Iterative Process: Steps 1 and 2 are repeated for each training sample in the dataset (or a mini-batch) during the training process. This process is known as stochastic gradient descent (SGD).\n",
    "\n",
    "Epochs: The entire dataset is passed through the network multiple times (epochs), allowing the model to learn from the data and gradually reduce the loss.\n",
    "\n",
    "Optimization Algorithm: The optimization algorithm (e.g., gradient descent, Adam) specifies how the parameters are updated based on the gradients. It can include techniques like momentum and adaptive learning rates.\n",
    "\n",
    "Convergence: The training process continues until a stopping criterion is met, such as a maximum number of epochs, early stopping based on validation loss, or achieving a desired level of accuracy.\n",
    "\n",
    "In summary, forward propagation computes predictions based on input data, and backpropagation calculates gradients to update the model's parameters. These processes are repeated iteratively until the neural network converges to a state where the loss is minimized, and the model can make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation class\n",
    "# Define activation functions (e.g., sigmoid, ReLU)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Activation:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron class\n",
    "# Define neuron properties and operations\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size, activation_func):\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "        self.activation_func = activation_func\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.activation_func(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer class\n",
    "# Define a layer of neurons and manage forward/backward passes\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_func):\n",
    "        self.neurons = [Neuron(input_size, activation_func) for _ in range(output_size)]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return [neuron.forward(inputs) for neuron in self.neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters class\n",
    "# Store and manage weights and biases\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.biases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "# Define the neural network architecture and methods for forward/backward passes\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LossFunction class\n",
    "# Implement loss functions (e.g., MSE, Cross-Entropy)\n",
    "\n",
    "class LossFunction:\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForwardProp class\n",
    "# Implement forward propagation logic\n",
    "\n",
    "class ForwardProp:\n",
    "    def forward_pass(self, model, inputs):\n",
    "        return model.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BackProp class\n",
    "# Implement backpropagation logic\n",
    "\n",
    "class BackProp:\n",
    "    def backward_pass(self, model, loss_func, inputs, targets, learning_rate):\n",
    "        # Backpropagation logic to update weights and biases\n",
    "        # Calculate gradients and update weights and biases\n",
    "        for layer in reversed(model.layers):\n",
    "            # Calculate gradients\n",
    "            gradients = []\n",
    "            for neuron in layer.neurons:\n",
    "                gradient = (neuron.activation_func(neuron.weights.dot(inputs) + neuron.bias) - targets) * \\\n",
    "                           neuron.activation_func(neuron.weights.dot(inputs) + neuron.bias) * \\\n",
    "                           (1 - neuron.activation_func(neuron.weights.dot(inputs) + neuron.bias))\n",
    "                gradients.append(gradient)\n",
    "\n",
    "            # Update weights and biases\n",
    "            for i, neuron in enumerate(layer.neurons):\n",
    "                neuron.weights -= learning_rate * gradients[i] * inputs\n",
    "                neuron.bias -= learning_rate * gradients[i]\n",
    "\n",
    "            inputs = [neuron.activation_func(neuron.weights.dot(inputs) + neuron.bias) for neuron in layer.neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradDescent class\n",
    "# Implement gradient descent or other optimization algorithms\n",
    "\n",
    "class GradDescent:\n",
    "    def stochastic_gradient_descent(self, model, loss_func, inputs, targets, learning_rate, epochs):\n",
    "        for _ in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = ForwardProp().forward_pass(model, inputs) \n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_func.mean_squared_error(targets, predictions)\n",
    "\n",
    "            # Backward pass and weight updates\n",
    "            BackProp().backward_pass(model, loss_func, inputs, targets, learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.0222645868202835\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,) and (3,) not aligned: 4 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m LossFunction()\n\u001b[1;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Training()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Use the trained model for predictions\u001b[39;00m\n\u001b[1;32m     29\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m])\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self, model, loss_func, optimizer, inputs, targets, learning_rate, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func\u001b[38;5;241m.\u001b[39mmean_squared_error(targets, predictions)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mBackProp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mBackProp.backward_pass\u001b[0;34m(self, model, loss_func, inputs, targets, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mneurons:\n\u001b[0;32m---> 11\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m (neuron\u001b[38;5;241m.\u001b[39mactivation_func(\u001b[43mneuron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mbias) \u001b[38;5;241m-\u001b[39m targets) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m     12\u001b[0m                neuron\u001b[38;5;241m.\u001b[39mactivation_func(neuron\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mdot(inputs) \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mbias) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m     13\u001b[0m                (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m neuron\u001b[38;5;241m.\u001b[39mactivation_func(neuron\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mdot(inputs) \u001b[38;5;241m+\u001b[39m neuron\u001b[38;5;241m.\u001b[39mbias))\n\u001b[1;32m     14\u001b[0m     gradients\u001b[38;5;241m.\u001b[39mappend(gradient)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Update weights and biases\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,) and (3,) not aligned: 4 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "class Training:\n",
    "    def train(self, model, loss_func, optimizer, inputs, targets, learning_rate, epochs):\n",
    "        for _ in range(epochs):\n",
    "            predictions = ForwardProp().forward_pass(model, inputs)\n",
    "            loss = loss_func.mean_squared_error(targets, predictions)\n",
    "            print(f\"Epoch {_ + 1}/{epochs}, Loss: {loss}\")\n",
    "            BackProp().backward_pass(model, loss_func, inputs, targets, learning_rate)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and train the neural network\n",
    "    input_size = 3\n",
    "    output_size = 2\n",
    "    activation_func = Activation()\n",
    "    \n",
    "    model = Model()\n",
    "    model.add_layer(Layer(input_size, 4, activation_func.sigmoid))\n",
    "    model.add_layer(Layer(4, output_size, activation_func.sigmoid))\n",
    "    \n",
    "    inputs = np.array([0.1, 0.2, 0.3])\n",
    "    targets = np.array([0.4, 0.5])\n",
    "    \n",
    "    optimizer = ForwardProp()\n",
    "    loss_func = LossFunction()\n",
    "    trainer = Training()\n",
    "    \n",
    "    trainer.train(model, loss_func, optimizer, inputs, targets, learning_rate=0.01, epochs=100)\n",
    "    \n",
    "    # Use the trained model for predictions\n",
    "    test_inputs = np.array([0.2, 0.3, 0.4])\n",
    "    predictions = model.forward(test_inputs)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class hierarchy design and software development\n",
    "\n",
    "So, now let's discuss the class hierarchy design and software development for the neural network implementation we have provided.\n",
    "\n",
    "Class Hierarchy Design:\n",
    "\n",
    "#### Activation:\n",
    "\n",
    "This class represents activation functions such as sigmoid, ReLU, etc.\n",
    "Provides methods for various activation functions.\n",
    "\n",
    "#### Neuron:\n",
    "\n",
    "Represents an individual neuron in a neural network.\n",
    "Has attributes for weights, bias, and an activation function.\n",
    "Performs the forward pass for a single neuron.\n",
    "\n",
    "#### Layer:\n",
    "\n",
    "Represents a layer of neurons.\n",
    "Contains multiple Neuron instances.\n",
    "Allows the creation of hidden layers in the neural network.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "Could be used to store and manage weights and biases for the entire network.\n",
    "Currently not implemented in the provided code.\n",
    "\n",
    "#### Model:\n",
    "\n",
    "Represents the neural network architecture.\n",
    "Contains multiple Layer instances.\n",
    "Performs the forward pass for the entire network.\n",
    "\n",
    "#### LossFunction:\n",
    "\n",
    "Represents the loss or error function used to measure the network's performance.\n",
    "Provides methods for calculating loss, e.g., mean squared error.\n",
    "\n",
    "#### ForwardProp:\n",
    "\n",
    "Implements the forward propagation logic for the network.\n",
    "Propagates inputs through the layers to generate predictions.\n",
    "\n",
    "#### BackProp:\n",
    "\n",
    "Implements the backpropagation logic for updating weights and biases.\n",
    "Calculates gradients and updates neuron parameters.\n",
    "\n",
    "#### GradDescent:\n",
    "\n",
    "Currently not implemented in the provided code.\n",
    "Typically used for gradient descent or other optimization algorithms.\n",
    "\n",
    "#### Training:\n",
    "\n",
    "Manages the training process, including data loading and training loops.\n",
    "Uses forward and backward propagation to update model parameters.\n",
    "Software Development:\n",
    "Object-Oriented Approach:\n",
    "\n",
    "The code follows an object-oriented approach, which is a good practice for organizing and encapsulating functionality.\n",
    "\n",
    "#### Modular Design:\n",
    "\n",
    "Each class is designed to have a specific responsibility, making the code modular and easier to maintain.\n",
    "\n",
    "#### Flexibility:\n",
    "\n",
    "The design allows flexibility in choosing activation functions, network architecture, and loss functions.\n",
    "\n",
    "#### Training Loop:\n",
    "\n",
    "The Training class manages the training process, including the forward and backward passes.\n",
    "It iterates over epochs, calculates loss, and updates model parameters.\n",
    "Extensibility:\n",
    "\n",
    "You can easily extend this code by adding new activation functions, loss functions, or optimization algorithms.\n",
    "\n",
    "#### Debugging:\n",
    "\n",
    "The modular structure makes it easier to debug and test individual components of the neural network.\n",
    "\n",
    "#### Usage Example:\n",
    "\n",
    "The code includes a usage example in the main section, demonstrating how to create and train a neural network.\n",
    "Overall, the provided class hierarchy design and software development approach are well-structured and suitable for building and training neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
